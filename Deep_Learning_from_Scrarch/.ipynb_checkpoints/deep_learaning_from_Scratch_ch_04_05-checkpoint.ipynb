{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습\n",
    "\n",
    "- `학습`이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 의미한다. 이번 장에서는 `신경망이 학습할 수 있도록 해주는 지표인 손실함수`에 대해 알아볼 것이다. 손실 함수의 결과값을 최소로 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다. \n",
    "\n",
    "### 데이터에서 학습한다!\n",
    "\n",
    "- `신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.` 데이터에서 학습한다는 것은 가중치 매개변수의 값을 `데이터를 보고 자동으로 결정한다는 의미이다.` 기계학습 방식에서는 feature를 사람이 설계하지만, 신경망은 `이미지에 포함된 중요한 특징까지도 기계가 스스로 학습할 것이다.`\n",
    "\n",
    "\n",
    "- 이미지에서 feature를 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 여기서 말하는 feature는 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다. 이미지의 feature는 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 feature를 많이 사용한다. 이런 feature를 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다. 하지만 `위의 방법들 조차도 feature를 만드는 것은 사람이므로 적합한 feature를 사용하지 않으면 좋은 결과를 얻을 수 없다.`\n",
    "\n",
    "\n",
    "- 신경망의 이점은 `모든 문제를 같은 맥락에서 풀 수 있다는 점이다.` 예를 들어 '5'를 인식하는 문제든, '개'를 인식하는 문제든, 아니면 '사람의 얼굴'을 인식하는 문제든, 세부사항과 관계없이 `신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도한다. 즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다.`\n",
    "\n",
    "### 손실함수(loss function)\n",
    "\n",
    "- 손실함수를 기준으로 최적의 매개변수 값을 탐색한다. 손실 함수는 `신경망 성능의 나쁨을 나타내는 지표`로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타낸다. \n",
    "\n",
    "#### MSE(Mean squared error,  평균 제곱 오차)\n",
    "\n",
    "$$E =  \\frac{1}{2} \\sum_{k}(y_{k} - t_{k})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy error(CEE, 교차 엔트로피 오차)\n",
    "\n",
    "- 정답일 때의 출력이 전체 값을 정하게 된다.\n",
    "\n",
    "$$E = - \\sum_{k}t_{k}\\log{y_{k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습\n",
    "\n",
    "기계학습 문제는 훈련 데이터를 사용해 학습한다. 더 구체적으로 말하면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. 이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것이다. 허나, 더 많은 데이터를 학습하려는 경우 일일이 손실 함수를 계산하는 것은 현실적이지 않을 것이다. 이런 경우 데이터 일부를 추려 전체의 \"근사치\"로 이용할 수 있다. `신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행`한다. 이 일부를 `미니배치(mini-batch)`라고 한다. \n",
    "\n",
    "- 예를 들어, 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 것이다. 이러한 학습 방법을 `미니배치 학습`이라고 한다. np.random.choice() 함수를 사용하여 랜덤으로 뽑아낸다. 예를 들면 np.random.choice(60000, 10)은 0이상 60000미만의 수 중에서 무작위로 10개를 골라낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "(x_train, label_train),(x_test, label_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(label_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "label_batch = label_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정답 레이블이 one-hot encoding이 아니라 '2'나 '7' 등의 숫자 레이블로 주어졌을때의 오차 구현하기\n",
    "\n",
    "- 이 구현에서는 `one-hot encoding일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 된다는 것이 핵심`이다.\n",
    "- 이 예에서는 y[np.arange(batch_size), t]는 만약 batch_size=5이고, t에 레이블이 [2, 7, 0, 9, 4]와 같이 저장되어 있다고 가정하면, [y[0, 2], y[1, 7], y[2, 0], y[3, 9], y[4, 4]]인 넘파이 배열을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 하강법\n",
    "\n",
    "- 일반적인 문제의 손실 함수는 매우 복잡하다. 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없다. 이런 상황에서 기울기를 잘 이용햐 함수의 최솟값(또는 가능한 한 작은값)을 찾으려는 것이 경사 하강법이다.\n",
    "\n",
    "\n",
    "- `기울기가 가리키는 쪽은 해당 지점에서의 함수의 출력 값을 가장 크게 줄이는 방향이다.` 그러나, 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 그쪽이 정말로 나아갈 방향인지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다. \n",
    "\n",
    "\n",
    "- 함수가 극솟값, 최솟값, 또 saddle point(안장점)이 되는 곳에서 기울기가 0이다. 극솟값은 국소적인 최소값, 즉 한정된 범위에서의 최솟값인 점이다. 안장점은 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점이다. 경사하강법은 기울기가 0인 지점인 곳을 찾지만 그것이 optimal solution이라고는 할 수 없다.(극솟값이나 안장점일 가능성이 있다.) 또, 복잡하고 찌그러진 모양의 함수라면 (대부분) 평평한 곳으로 파고들면서 Plateau(고원)이라 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성하며 원소들이 모두 0인 배열\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h)계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h)계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수의 기울기는 numerical_gradient(f,x)로 구하고, 그 기울기에 lr을 곱한 값으로 갱신하는 처리를 step_num이 반복한다.\n",
    "\n",
    "- 학습률 같은 매개변수를 `하이퍼 파라미터(hyper parameter)`라고 합니다. 이는 가중치와 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수인 반면, `학습률 같은 하이퍼 파라미터는 사람이 직접 설정해야하는 매개변수인 것이다.` 일반적으로는 이 하이퍼 파라미터들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    \"\"\"\n",
    "    --------------------------------------\n",
    "    f : 최적화하려는 함수\n",
    "    init_x : 초기값\n",
    "    lr : learning rate\n",
    "    step_num : 경사법에 따른 반복 횟수\n",
    "    --------------------------------------\n",
    "    \"\"\"\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient_1d(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 : 경사하강법으로 $f(x_{0}, x_{1}) = x_{0}^2 + x_{1}^2$의 최솟값을 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(function_2, init_x=np.array([-3.0, 4.0]), lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02950509  1.31157278  0.18445031]\n",
      " [-1.10098583  1.01354886  0.20636394]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0085903   1.69913764  0.29639774]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6747571905980798"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03048343  0.45710417 -0.4875876 ]\n",
      " [ 0.04572515  0.68565625 -0.7313814 ]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dW의 내용을 보면, 예를 들어 $\\frac{\\delta L}{\\delta W}$의 $\\frac{\\delta L}{\\delta W_{11}}$은 대략 0.1이다. \n",
    "### 이는 $W_{11}$을 h만큼 늘리면 손실함수의 값은 0.2h만큼 증가한다는 의미이다.\n",
    "\n",
    "마찬가지로 $\\frac{\\delta L}{\\delta W_{13}}$은 대략 -0.4이므로 $\\frac{\\delta L}{\\delta W_{13}}$을 h만큼 늘리면 손실 함수의 값은 0.5h만큼 감소하는 것이다. `그래서 손실 함수를 줄인다는 관점에서는` $W_{23}$ `은 양의 방향으로 갱신하고,` $W_{11}$ `은 음의 방향으로 갱신해야 함을 알 수 있다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현하기\n",
    "\n",
    "신경망의 학습 절차는 다음과 같다. 아래는 `경사하강법으로 매개변수를 갱신하는 방법이며 미니배치로 데이터를 무작위로 선정하기 때문에 SGD(Stochastic Gradient Descent)라고 부른다. 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법이라는 의미이다.`\n",
    "\n",
    "#### 전제\n",
    "- 신경망에는 적응 가능한 가중치와 편향이있고 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
    "\n",
    "#### 1단계 - 미니배치\n",
    "- 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
    "\n",
    "#### 2단계 - 기울기 산출\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "#### 3단계 - 매개변수 갱신\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "\n",
    "#### 4단계 - 반복\n",
    "- 1~3단계 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : 입력 데이터, label : 정답 레이블\n",
    "    def loss(self, x, label):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        y = np.argmax(label, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == label) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, label):\n",
    "        loss_W = lambda x : self.loss(x, label)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10218333333333333, 0.101\n",
      "train acc, test acc | 0.7715, 0.78\n",
      "train acc, test acc | 0.8773166666666666, 0.8818\n",
      "train acc, test acc | 0.8978833333333334, 0.9028\n",
      "train acc, test acc | 0.9070833333333334, 0.9107\n",
      "train acc, test acc | 0.9134333333333333, 0.9141\n",
      "train acc, test acc | 0.9184, 0.921\n",
      "train acc, test acc | 0.9229666666666667, 0.9252\n",
      "train acc, test acc | 0.9265333333333333, 0.929\n",
      "train acc, test acc | 0.9293666666666667, 0.9316\n",
      "train acc, test acc | 0.9335, 0.9337\n",
      "train acc, test acc | 0.9359, 0.9363\n",
      "train acc, test acc | 0.9387, 0.9397\n",
      "train acc, test acc | 0.94155, 0.9413\n",
      "train acc, test acc | 0.9433333333333334, 0.9434\n",
      "train acc, test acc | 0.9449833333333333, 0.9443\n",
      "train acc, test acc | 0.9470166666666666, 0.9456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, label_train), (x_test, label_test)= load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list =[]\n",
    "\n",
    "#hyper parameter\n",
    "iters_num = 10000 # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # mini-batch size\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iter_per_epoch = max(train_size/ batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # mini-batch \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    label_batch = label_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "#     grad =  network.numerical_gradient(x_batch, label_batch) # 이건 너무 느림 진짜!!!\n",
    "    grad = network.gradient(x_batch, label_batch) # 성능 개선판!\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        network.params[key] -= learning_rate *grad[key]\n",
    "        \n",
    "    # 학습경과 기록\n",
    "    loss = network.loss(x_batch, label_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1 epoch당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, label_train)\n",
    "        test_acc = network.accuracy(x_test, label_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시험 데이터로 평가하기 \n",
    "- 훈련 데이터의 mini-batch에 대한 손실 함수의 값이 서서히 내려가는 것을 확인할 수 있다. 신경망이 잘 학습하고 있다는 것이다. 그러나 `overfitting을 일으키지 않는지 확인해야한다.` overfitting이 되었다는 것은, 예를 들어 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻이다. `우리의 원래 목표는 범용적인 능력을 익히는 것이다.`\n",
    "\n",
    "\n",
    "- 아래에서는 1epoch(훈련 데이터를 모두 소진했을 때의 횟수에 해당.예를 들어 훈련 데이터 10,000개를 100개의 mini-batch로 학습할 경우, SGD를 100회 반복하면 모든 훈련 데이터를 소진하게 되는데 이 경우 100회가 1epoch이다.)별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록한다. 그 이유는 매번 for문 안에서 계산하기에는 시간이 걸리며 자주 기록할 필요는 없기 때문이다. 추세를 알 수 있으면 충분!! \n",
    "\n",
    "\n",
    "- 아래의 그래프로 보아 위의 학습에서는 overfitting이 일어나지 않았음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 270,
       "width": 392
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오차역전파법(Backpropagation)\n",
    "\n",
    "수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 것이 단점이다. 이러한 단점을 보완하는 가중치 매개변수의 기울기를 효율적으로 계산하는 `오차역전파법(Backpropagation)`\n",
    "\n",
    "- 계산 그래프의 특징은 `국소적 계산`을 전파함으로써 최종결과를 얻는다는 점이다. 즉, 각 노드는 자신과 관련한 계산 외에는 아무것도 신경 쓸 것이 없다. 또한 `중간 계산 결과를 모두 보관`할 수 있다. 이런 특징을 바탕으로 `실제 계산 그래프를 사용하는 가장 큰 이유는 역전파를 통해 미분을 효율적으로 계산할 수 있는 점에 있다.`\n",
    "\n",
    "- 덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않는다. 다만, 곱세의 역전파는 순방향 입력 신호의 값이 필요하다. 서로 입력된 것들을 반대로 곱해 주어야 하기 때문이다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순한 계층 구현\n",
    "#### 곱셈 계층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 덧셈 계층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수 계층 구현하기\n",
    "#### ReLU 계층\n",
    "\n",
    "순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다. 반면, 순전파 때 x가 0이하이면 역전파 때는 하류로 신호를 보내지 않는다.\n",
    "\n",
    "$$y=\\begin{cases}\n",
    "   x & x  > 0 \\\\\n",
    "   0 & x \\leq 0\n",
    "   \\end{cases}$$\n",
    "\n",
    "$$\\frac{\\delta y}{\\delta x}=\\begin{cases}\n",
    "   1 & x  > 0 \\\\\n",
    "   0 & x \\leq 0\n",
    "   \\end{cases}$$\n",
    "   \n",
    "- ReLU 계층은 전기 회로의 '스위치'에 비유할 수 있다. 순전파 때 전류가 흐르고 있으면 스위치를 ON으로 하고, 흐르지 않으면 OFF로 한다. 역전파 때는 스위치가 ON이라면 전류가 그대로 흐르고, OFF면 더 이상 흐르지 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "mask = x<=0\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid 계층\n",
    "\n",
    "$$y = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine / Softmax 계층 구현하기\n",
    "\n",
    "\n",
    "#### Affine 계층\n",
    "\n",
    "신경망의 순전파에서는 가중치 신호의 총합을 게산하기 때문에 행렬의 곱을 사용했다. 그러한 행렬의 곱을 기하학에서는 `어파인 변환(Affine transformation)`이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax-with-Loss 계층\n",
    "\n",
    "신경망에서 수행하는 작업은 학습과 추론 두가지이다. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. 신경망은 추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다. 또한 `신경망에서 정규화하지 않는 출력 결과를 점수(Score)라고 한다.` \n",
    "\n",
    "- 즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되니 Softmax 계층은 필요 없다는 것이다. 반면, 신경망을 학습할 때는 Softmax계층이 필요하다.\n",
    "\n",
    "- 아래의 코드에서 `역전파 때에는 전파하는 값을 배치의 수(batch_size)로 나눠서 데이터 1개당 오차를 앞 계층으로 전파하는 점에 주의하자!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation을 적용한 신경망 구현\n",
    "\n",
    "- 계층을 사용함으로써 인식결과를 얻는 처리(predict())와 기울기를 구하는 처리(gradient()) 계층의 전파만으로 동작이 이루어질 수 있다.\n",
    "\n",
    "- OrderedDict를 사용하여 순전파 때는 추가한 순서대로 각 계층의 forward() 메서드를 호출하기만 하면 처리가 완료된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
